<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TwinTalk - Meeting History</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary: #1a2a6c;
            --secondary: #4a569d;
            --accent: #00b4db;
            --accent-light: #00c6ff;
            --light: #f8f9fa;
            --dark: #2d3748;
            --gray: #718096;
            --success: #10b981;
            --error: #ef4444;
            --warning: #f59e0b;
            --gradient-primary: linear-gradient(135deg, var(--primary), var(--secondary));
            --gradient-accent: linear-gradient(135deg, var(--accent), var(--accent-light));
            --shadow-sm: 0 4px 6px rgba(0, 0, 0, 0.05);
            --shadow-md: 0 10px 15px rgba(0, 0, 0, 0.1);
            --shadow-lg: 0 20px 25px rgba(0, 0, 0, 0.15);
            --border-radius: 12px;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Inter', sans-serif;
        }

        body {
            background: linear-gradient(135deg, #1a2a6c 0%, #4a569d 50%, #00b4db 100%);
            color: var(--dark);
            line-height: 1.6;
            min-height: 100vh;
        }

        .container {
            width: 90%;
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem 15px;
        }

        /* Header */
        header {
            background: rgba(255, 255, 255, 0.95);
            color: var(--dark);
            padding: 1rem 0;
            box-shadow: var(--shadow-md);
            backdrop-filter: blur(10px);
        }

        .navbar {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-size: 1.5rem;
            font-weight: 700;
            display: flex;
            align-items: center;
            color: var(--primary);
            text-decoration: none;
        }

        .logo i {
            margin-right: 10px;
        }

        .nav-links {
            display: flex;
            gap: 2rem;
            align-items: center;
        }

        .nav-links a {
            text-decoration: none;
            color: var(--dark);
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .nav-links a:hover {
            color: var(--accent);
        }

        .nav-links a.active {
            color: var(--accent);
            font-weight: 600;
        }

        /* Main Content */
        .history-container {
            background: rgba(255, 255, 255, 0.95);
            border-radius: var(--border-radius);
            padding: 2rem;
            box-shadow: var(--shadow-lg);
            backdrop-filter: blur(10px);
        }

        .page-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 2px solid var(--light);
        }

        .page-title {
            font-size: 2rem;
            font-weight: 700;
            color: var(--primary);
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .page-title i {
            color: var(--accent);
        }

        .refresh-btn {
            background: var(--accent);
            color: white;
            border: none;
            padding: 0.8rem 1.5rem;
            border-radius: var(--border-radius);
            cursor: pointer;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            transition: all 0.3s ease;
        }

        .refresh-btn:hover {
            background: var(--primary);
            transform: translateY(-2px);
        }

        /* History List */
        .history-list {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }

        .history-item {
            background: var(--light);
            border-radius: var(--border-radius);
            padding: 1.5rem;
            border-left: 4px solid var(--accent);
            transition: all 0.3s ease;
        }

        .history-item:hover {
            transform: translateY(-2px);
            box-shadow: var(--shadow-md);
        }

        .history-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 1rem;
        }

        .history-info h3 {
            font-size: 1.2rem;
            font-weight: 600;
            color: var(--primary);
            margin-bottom: 0.5rem;
        }

        .history-meta {
            display: flex;
            gap: 1rem;
            font-size: 0.9rem;
            color: var(--gray);
        }

        .history-actions {
            display: flex;
            gap: 0.5rem;
        }

        .action-btn {
            padding: 0.5rem 1rem;
            border: none;
            border-radius: var(--border-radius);
            cursor: pointer;
            font-weight: 600;
            font-size: 0.8rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            transition: all 0.3s ease;
        }

        .action-btn.transcribe {
            background: var(--accent);
            color: white;
        }

        .action-btn.summary {
            background: var(--success);
            color: white;
        }

        .action-btn.download {
            background: var(--primary);
            color: white;
        }

        .action-btn.convert {
            background: #8b5cf6;
            color: white;
        }

        .action-btn.delete {
            background: var(--error);
            color: white;
        }

        .action-btn.audio {
            background: var(--secondary);
            color: white;
        }

        .action-btn:hover {
            transform: translateY(-1px);
            box-shadow: var(--shadow-sm);
        }

        .action-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }

        /* Content Sections */
        .content-section {
            margin-top: 1rem;
            padding: 1rem;
            background: white;
            border-radius: var(--border-radius);
            border: 1px solid #e2e8f0;
        }

        .section-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid #e2e8f0;
        }

        .section-title {
            font-weight: 600;
            color: var(--primary);
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .section-content {
            line-height: 1.6;
            color: var(--dark);
        }

        .transcription-content {
            max-height: 200px;
            overflow-y: auto;
            white-space: pre-wrap;
            font-family: 'Inter', sans-serif;
        }

        .summary-content {
            background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
            padding: 1rem;
            border-radius: var(--border-radius);
            border-left: 4px solid var(--success);
        }

        /* Status Badges */
        .status-badge {
            display: inline-flex;
            align-items: center;
            gap: 0.3rem;
            padding: 0.3rem 0.6rem;
            border-radius: 20px;
            font-size: 0.7rem;
            font-weight: 600;
            margin-left: 0.5rem;
        }

        .status-badge.pending {
            background: #fef3c7;
            color: #92400e;
        }

        .status-badge.processing {
            background: #dbeafe;
            color: #1e40af;
        }

        .status-badge.completed {
            background: #d1fae5;
            color: #065f46;
        }

        .status-badge.failed {
            background: #fee2e2;
            color: #991b1b;
        }

        .status-badge.ai {
            background: #e0f2fe;
            color: #0369a1;
        }

        /* Empty State */
        .empty-state {
            text-align: center;
            padding: 3rem 2rem;
            color: var(--gray);
        }

        .empty-icon {
            font-size: 3rem;
            margin-bottom: 1rem;
            color: #ddd;
        }

        .empty-state h3 {
            font-size: 1.5rem;
            margin-bottom: 0.5rem;
            color: var(--gray);
        }

        /* Loading States */
        .loading {
            opacity: 0.6;
            pointer-events: none;
        }

        .loading-spinner {
            display: inline-block;
            width: 1rem;
            height: 1rem;
            border: 2px solid #ffffff;
            border-radius: 50%;
            border-top-color: transparent;
            animation: spin 1s ease-in-out infinite;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }
        /* Notification */
        .notification {
            position: fixed;
            top: 20px;
            right: 20px;
            padding: 15px 20px;
            border-radius: 8px;
            color: white;
            font-weight: 500;
            z-index: 10000;
            box-shadow: var(--shadow-md);
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            gap: 10px;
            max-width: 350px;
        }

        .notification.success {
            background: var(--success);
        }

        .notification.error {
            background: var(--error);
        }

        .notification.warning {
            background: var(--warning);
        }

        .notification.hidden {
            opacity: 0;
            transform: translateX(100%);
        }

        /* Modal */
        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 1000;
            align-items: center;
            justify-content: center;
        }

        .modal-content {
            background: white;
            border-radius: var(--border-radius);
            padding: 2rem;
            width: 90%;
            max-width: 500px;
            box-shadow: var(--shadow-lg);
        }

        .modal-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1.5rem;
        }

        .modal-title {
            font-size: 1.5rem;
            font-weight: 600;
            color: var(--primary);
        }

        .close-modal {
            background: none;
            border: none;
            font-size: 1.5rem;
            cursor: pointer;
            color: var(--gray);
        }

        .modal-body {
            margin-bottom: 1.5rem;
        }

        .modal-footer {
            display: flex;
            justify-content: flex-end;
            gap: 1rem;
        }

        /* Audio Player */
        .audio-player {
            width: 100%;
            margin: 1rem 0;
        }

        .audio-player audio {
            width: 100%;
            border-radius: var(--border-radius);
        }

        /* Azure Speech Status */
        .azure-status {
            background: #f0f9ff;
            border: 1px solid #e0f2fe;
            border-radius: var(--border-radius);
            padding: 1rem;
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
        }

        .azure-status.connected {
            background: #f0fdf4;
            border-color: #dcfce7;
            color: #166534;
        }

        .azure-status.disconnected {
            background: #fef2f2;
            border-color: #fecaca;
            color: #dc2626;
        }

        /* FFmpeg Status */
        .ffmpeg-status {
            background: #faf5ff;
            border: 1px solid #e9d5ff;
            border-radius: var(--border-radius);
            padding: 1rem;
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.9rem;
        }

        .ffmpeg-status.connected {
            background: #f0fdf4;
            border-color: #dcfce7;
            color: #166534;
        }

        .ffmpeg-status.loading {
            background: #fef3c7;
            border-color: #fcd34d;
            color: #92400e;
        }

        .ffmpeg-status.disconnected {
            background: #fef2f2;
            border-color: #fecaca;
            color: #dc2626;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .history-header {
                flex-direction: column;
                gap: 1rem;
            }

            .history-actions {
                width: 100%;
                justify-content: flex-start;
                flex-wrap: wrap;
            }

            .action-btn {
                flex: 1;
                min-width: 120px;
                justify-content: center;
            }

            .nav-links {
                gap: 1rem;
            }
        }
    </style>
</head>
<body>
    <!-- Header -->
    <header>
        <div class="container">
            <nav class="navbar">
                <a href="index.html" class="logo">
                    <i class="fas fa-comments"></i>
                    TwinTalk
                </a>
                <div class="nav-links">
                    <a href="index.html">Home</a>
                    <a href="meeting.html">Meeting</a>
                    <a href="history.html" class="active">History</a>
                </div>
            </nav>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="history-container">
            <div class="page-header">
                <h1 class="page-title">
                    <i class="fas fa-history"></i>
                    Meeting History & Recordings
                </h1>
                <button class="refresh-btn" id="refreshHistory">
                    <i class="fas fa-sync-alt"></i>
                    Refresh
                </button>
            </div>

            <!-- FFmpeg Status Indicator -->
            <div class="ffmpeg-status" id="ffmpegStatus">
                <i class="fas fa-circle-notch fa-spin"></i>
                <span>Loading audio conversion tools...</span>
            </div>

            <!-- Azure Speech Services Status Indicator -->
            <div class="azure-status" id="azureStatus">
                <i class="fas fa-circle-notch fa-spin"></i>
                <span>Checking Azure Speech Services status...</span>
            </div>

            <!-- Azure Language Service Status Indicator -->
            <div class="azure-status" id="azureLanguageStatus">
                <i class="fas fa-circle-notch fa-spin"></i>
                <span>Checking Azure Language Service status...</span>
            </div>

            <div class="history-list" id="historyList">
                <!-- History items will be loaded here -->
            </div>
        </div>
    </div>

    <!-- Notification -->
    <div class="notification hidden" id="notification"></div>

    <!-- Delete Confirmation Modal -->
    <div class="modal" id="deleteModal">
        <div class="modal-content">
            <div class="modal-header">
                <h3 class="modal-title">Confirm Deletion</h3>
                <button class="close-modal" id="closeDeleteModal">&times;</button>
            </div>
            <div class="modal-body">
                <p>Are you sure you want to delete this recording and all its data? This action cannot be undone.</p>
            </div>
            <div class="modal-footer">
                <button class="action-btn" id="cancelDelete">Cancel</button>
                <button class="action-btn delete" id="confirmDelete">Delete</button>
            </div>
        </div>
    </div>

    <!-- Format Selection Modal -->
    <div class="modal" id="formatModal">
        <div class="modal-content">
            <div class="modal-header">
                <h3 class="modal-title">Select Download Format</h3>
                <button class="close-modal" id="closeFormatModal">&times;</button>
            </div>
            <div class="modal-body">
                <p>Choose the format you want to download:</p>
                <div style="display: flex; flex-direction: column; gap: 1rem; margin-top: 1rem;">
                    <button class="action-btn convert" id="downloadMP4" style="justify-content: center;">
                        <i class="fas fa-file-video"></i>
                        MP4 Format (Better Compatibility)
                    </button>
                    <button class="action-btn download" id="downloadWebM" style="justify-content: center;">
                        <i class="fas fa-file-audio"></i>
                        Original WebM Format
                    </button>
                </div>
            </div>
        </div>
    </div>

    <!-- FFmpeg Script -->
    <script src="https://unpkg.com/@ffmpeg/ffmpeg@0.12.4/dist/ffmpeg.min.js"></script>
    <!-- Azure Speech SDK -->
    <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
    <script>
        // DOM Elements
        const historyList = document.getElementById('historyList');
        const refreshHistory = document.getElementById('refreshHistory');
        const notification = document.getElementById('notification');
        const deleteModal = document.getElementById('deleteModal');
        const closeDeleteModal = document.getElementById('closeDeleteModal');
        const cancelDelete = document.getElementById('cancelDelete');
        const confirmDelete = document.getElementById('confirmDelete');
        const azureStatus = document.getElementById('azureStatus');
        const azureLanguageStatus = document.getElementById('azureLanguageStatus');
        const ffmpegStatus = document.getElementById('ffmpegStatus');
        const formatModal = document.getElementById('formatModal');
        const closeFormatModal = document.getElementById('closeFormatModal');
        const downloadMP4 = document.getElementById('downloadMP4');
        const downloadWebM = document.getElementById('downloadWebM');

        // State
        let userEmail = '';
        let recordings = [];
        let recordingToDelete = null;
        let recordingToDownload = null;
        let azureSpeechAvailable = false;
        let azureLanguageAvailable = false;
        let ffmpeg = null;
        let isFFmpegLoaded = false;

        // Azure Speech Services Configuration
        const AZURE_KEYS = {
            key1: "49AhVrABTgJDUjQGN8PhPLjCquisNWo0YDsFSlN5QDonBwMddKWyJQQJ99BKACNns7RXJ3w3AAAYACOGgZIN",
            key2: "Edvsahl7u2yDOWvIZac1EXW6kVZUVty4c8mEH2qnk2taLcBPfehwJQQJ99BKACNns7RXJ3w3AAAYACOGgvNo"
        };
        
        const AZURE_REGION = "koreacentral";

        // Azure Language Service Configuration for Summarization
        const AZURE_LANGUAGE_KEY = "5m9TwdPIivDM9FfyrCONG9uti4deTIDAo9eT6iJvNhIyBjOnLkpSJQQJ99BKACNns7RXJ3w3AAAaACOGunRN";
        const AZURE_LANGUAGE_ENDPOINT = "https://twin-talk-language.cognitiveservices.azure.com/";

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            loadUserEmail();
            loadHistory();
            setupEventListeners();
            highlightSelectedRecording();
            checkAzureSpeechStatus();
            checkAzureLanguageStatus();
            initializeFFmpeg();
        });

        // Setup event listeners
        function setupEventListeners() {
            refreshHistory.addEventListener('click', loadHistory);
            closeDeleteModal.addEventListener('click', closeDeleteModalHandler);
            cancelDelete.addEventListener('click', closeDeleteModalHandler);
            confirmDelete.addEventListener('click', deleteRecordingConfirmed);
            closeFormatModal.addEventListener('click', closeFormatModalHandler);
            downloadMP4.addEventListener('click', downloadAsMP4);
            downloadWebM.addEventListener('click', downloadAsWebM);
        }

        // Initialize FFmpeg with fallback to Web Audio API
        async function initializeFFmpeg() {
            try {
                ffmpegStatus.innerHTML = '<i class="fas fa-circle-notch fa-spin"></i> Loading audio conversion tools...';
                
                // Try to load FFmpeg
                const { createFFmpeg } = FFmpeg;
                ffmpeg = createFFmpeg({ 
                    log: false,
                    corePath: 'https://unpkg.com/@ffmpeg/core@0.11.0/dist/ffmpeg-core.js'
                });
                
                await ffmpeg.load();
                isFFmpegLoaded = true;
                
                ffmpegStatus.className = 'ffmpeg-status connected';
                ffmpegStatus.innerHTML = '<i class="fas fa-check-circle"></i> Audio conversion ready (FFmpeg)';
                console.log('✅ FFmpeg loaded successfully');
                
            } catch (error) {
                console.error('FFmpeg loading failed, using Web Audio API:', error);
                isFFmpegLoaded = false;
                ffmpegStatus.className = 'ffmpeg-status disconnected';
                ffmpegStatus.innerHTML = '<i class="fas fa-exclamation-triangle"></i> Using Web Audio API for conversion';
                showNotification('Using Web Audio API for audio conversion', 'warning');
            }
        }

        // Web Audio API conversion functions
        function audioBufferToWav(buffer) {
            const numChannels = buffer.numberOfChannels;
            const sampleRate = buffer.sampleRate;
            const format = 1; // PCM
            const bitDepth = 16;
            
            const bytesPerSample = bitDepth / 8;
            const blockAlign = numChannels * bytesPerSample;
            
            const bufferLength = buffer.length * numChannels * bytesPerSample;
            const wavBuffer = new ArrayBuffer(44 + bufferLength);
            const view = new DataView(wavBuffer);
            
            // Write WAV header
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + bufferLength, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, format, true);
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * blockAlign, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitDepth, true);
            writeString(view, 36, 'data');
            view.setUint32(40, bufferLength, true);
            
            // Write audio data
            let offset = 44;
            for (let i = 0; i < buffer.length; i++) {
                for (let channel = 0; channel < numChannels; channel++) {
                    const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
                    view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
                    offset += 2;
                }
            }
            
            return wavBuffer;
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        // Convert WebM to WAV using Web Audio API
        async function convertWebMToWAV(webmBlob) {
            return new Promise((resolve, reject) => {
                try {
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const fileReader = new FileReader();
                    
                    fileReader.onload = function() {
                        audioContext.decodeAudioData(fileReader.result)
                            .then(audioBuffer => {
                                const wavBuffer = audioBufferToWav(audioBuffer);
                                const wavBlob = new Blob([wavBuffer], { type: 'audio/wav' });
                                resolve(wavBlob);
                                audioContext.close();
                            })
                            .catch(error => {
                                console.error('Audio decoding error:', error);
                                reject(new Error('Failed to decode audio: ' + error.message));
                            });
                    };
                    
                    fileReader.onerror = () => {
                        reject(new Error('Failed to read audio file'));
                    };
                    
                    fileReader.readAsArrayBuffer(webmBlob);
                } catch (error) {
                    reject(new Error('Web Audio API not supported: ' + error.message));
                }
            });
        }

        // Convert WebM to WAV using FFmpeg (if available)
        async function convertWebMToWAVWithFFmpeg(webmBlob) {
            if (!isFFmpegLoaded || !ffmpeg) {
                throw new Error('FFmpeg not loaded');
            }

            try {
                // Convert blob to Uint8Array
                const arrayBuffer = await webmBlob.arrayBuffer();
                const uint8Array = new Uint8Array(arrayBuffer);
                
                // Write WebM file to FFmpeg
                ffmpeg.FS('writeFile', 'input.webm', uint8Array);
                
                // Convert to WAV (16kHz, 16-bit, mono - optimal for speech recognition)
                await ffmpeg.run(
                    '-i', 'input.webm',
                    '-acodec', 'pcm_s16le',
                    '-ac', '1',
                    '-ar', '16000',
                    'output.wav'
                );
                
                // Read the converted file
                const data = ffmpeg.FS('readFile', 'output.wav');
                const wavBlob = new Blob([data.buffer], { type: 'audio/wav' });
                
                // Clean up
                ffmpeg.FS('unlink', 'input.webm');
                ffmpeg.FS('unlink', 'output.wav');
                
                console.log('✅ WebM to WAV conversion successful with FFmpeg');
                return wavBlob;
                
            } catch (error) {
                console.error('❌ FFmpeg conversion failed:', error);
                throw error;
            }
        }

        // Main conversion function that tries both methods
        async function convertWebMToWAVForAzure(webmBlob) {
            // Try FFmpeg first if available
            if (isFFmpegLoaded) {
                try {
                    return await convertWebMToWAVWithFFmpeg(webmBlob);
                } catch (error) {
                    console.log('FFmpeg conversion failed, falling back to Web Audio API');
                }
            }
            
            // Fallback to Web Audio API
            return await convertWebMToWAV(webmBlob);
        }

        // Convert WebM to MP4 using FFmpeg (for downloads)
        async function convertWebMToMP4(webmBlob, recordingId) {
            if (!isFFmpegLoaded || !ffmpeg) {
                throw new Error('FFmpeg not loaded');
            }

            try {
                showNotification('Converting to MP4 format...', 'success');

                // Convert blob to Uint8Array
                const arrayBuffer = await webmBlob.arrayBuffer();
                const uint8Array = new Uint8Array(arrayBuffer);
                
                // Write WebM file to FFmpeg
                ffmpeg.FS('writeFile', 'input.webm', uint8Array);
                
                // Convert to MP4 (copy codec for fast conversion)
                await ffmpeg.run('-i', 'input.webm', '-c', 'copy', 'output.mp4');
                
                // Read the converted file
                const data = ffmpeg.FS('readFile', 'output.mp4');
                const mp4Blob = new Blob([data.buffer], { type: 'video/mp4' });
                
                // Clean up
                ffmpeg.FS('unlink', 'input.webm');
                ffmpeg.FS('unlink', 'output.mp4');
                
                console.log('✅ Conversion successful');
                return mp4Blob;
                
            } catch (error) {
                console.error('❌ Conversion failed:', error);
                
                // Clean up on error
                try {
                    ffmpeg.FS('unlink', 'input.webm');
                    ffmpeg.FS('unlink', 'output.mp4');
                } catch (e) {
                    // Ignore cleanup errors
                }
                
                throw error;
            }
        }

        // Load user email from localStorage
        function loadUserEmail() {
            const userData = JSON.parse(localStorage.getItem('userData') || '{}');
            userEmail = userData.email || 'demo@user.com';
            
            if (!userEmail) {
                showNotification('Please sign in to view history', 'warning');
            }
        }

        // Load history from localStorage
        function loadHistory() {
            try {
                historyList.classList.add('loading');
                
                // Get recordings from localStorage
                recordings = JSON.parse(localStorage.getItem('meetingRecordings') || '[]');
                recordings.sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));
                
                displayHistory(recordings);
                
            } catch (error) {
                console.error('Error loading history:', error);
                showNotification('Failed to load history', 'error');
                displayEmptyState();
            } finally {
                historyList.classList.remove('loading');
            }
        }

        // Display empty state
        function displayEmptyState() {
            historyList.innerHTML = `
                <div class="empty-state">
                    <div class="empty-icon">
                        <i class="fas fa-microphone-slash"></i>
                    </div>
                    <h3>No Recordings Yet</h3>
                    <p>Your meeting recordings will appear here after you record meetings</p>
                    <button class="refresh-btn" onclick="window.location.href='meeting.html'" style="margin-top: 1rem;">
                        <i class="fas fa-video"></i>
                        Start a Meeting
                    </button>
                </div>
            `;
        }

        // Enhanced display history items
        function displayHistory(history) {
            if (history.length === 0) {
                displayEmptyState();
                return;
            }

            historyList.innerHTML = history.map(item => {
                const hasAudio = localStorage.getItem(`audio_${item.id}`) !== null;
                const hasTranscription = item.transcription && item.transcription.trim() !== '';
                const hasSummary = item.summary && item.summary.trim() !== '';
                const isRealTranscript = item.isRealTranscript;
                const isRealSummary = item.isRealSummary;
                const participantCount = item.participantCount || 1;
                const includesAllParticipants = item.includesAllParticipants || false;
                
                return `
                <div class="history-item" data-id="${item.id}" id="recording-${item.id}">
                    <div class="history-header">
                        <div class="history-info">
                            <h3>${item.fileName || 'Meeting Recording'}
                                ${hasTranscription ? 
                                    (isRealTranscript ? 
                                        '<span class="status-badge ai"><i class="fas fa-robot"></i> Azure AI Transcribed</span>' : 
                                        '<span class="status-badge completed"><i class="fas fa-check"></i> Transcribed</span>') : 
                                    hasAudio ? 
                                    '<span class="status-badge pending"><i class="fas fa-microphone"></i> Recorded</span>' : 
                                    '<span class="status-badge failed"><i class="fas fa-exclamation-triangle"></i> No Audio</span>'}
                                ${hasSummary ? 
                                    (isRealSummary ? 
                                        '<span class="status-badge ai"><i class="fas fa-robot"></i> Azure AI Summarized</span>' : 
                                        '<span class="status-badge completed"><i class="fas fa-file-alt"></i> Summarized</span>') : ''}
                                ${includesAllParticipants ? '<span class="status-badge completed"><i class="fas fa-users"></i> All Participants</span>' : ''}
                            </h3>
                            <div class="history-meta">
                                <span><i class="fas fa-calendar"></i> ${formatDate(item.timestamp)}</span>
                                <span><i class="fas fa-clock"></i> ${formatTime(item.duration || 0)}</span>
                                <span><i class="fas fa-hdd"></i> ${formatFileSize(item.size || 0)}</span>
                                <span><i class="fas fa-users"></i> ${participantCount} Participants</span>
                                ${item.roomId ? `<span><i class="fas fa-users"></i> Room: ${item.roomId}</span>` : ''}
                                ${item.userName ? `<span><i class="fas fa-user"></i> ${item.userName}</span>` : ''}
                            </div>
                        </div>
                        <div class="history-actions">
                            ${hasAudio ? `
                                <button class="action-btn audio" onclick="playRecording('${item.id}')">
                                    <i class="fas fa-circle"></i>
                                    
                                </button>
                            ` : ''}
                            
                            ${hasAudio && !hasTranscription ? `
                                <button class="action-btn transcribe" onclick="transcribeRecordingWithAzure('${item.id}')" ${!azureSpeechAvailable ? 'disabled' : ''}>
                                    <i class="fas fa-keyboard"></i>
                                    ${azureSpeechAvailable ? 'Transcribe' : 'Azure Offline'}
                                </button>
                            ` : ''}
                            
                            ${hasTranscription && !hasSummary ? `
                                <button class="action-btn summary" onclick="generateSummary('${item.id}')" ${!azureLanguageAvailable ? 'disabled' : ''}>
                                    <i class="fas fa-file-alt"></i>
                                    ${azureLanguageAvailable ? 'AI Summary' : 'Azure Offline'}
                                </button>
                            ` : ''}
                            
                            ${hasAudio ? `
                                <button class="action-btn convert" onclick="showFormatSelection('${item.id}')" ${!isFFmpegLoaded ? 'disabled' : ''}>
                                    <i class="fas fa-file-video"></i>
                                    ${isFFmpegLoaded ? 'Download' : 'No Conversion'}
                                </button>
                            ` : ''}
                            
                            <button class="action-btn delete" onclick="confirmDeleteRecording('${item.id}')">
                                <i class="fas fa-trash"></i>
                                Delete
                            </button>
                        </div>
                    </div>

                    ${hasAudio ? `
                        <div class="audio-player">
                            <audio controls id="audio-${item.id}">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    ` : ''}

                    ${hasTranscription ? `
                        <div class="content-section">
                            <div class="section-header">
                                <h4 class="section-title">
                                    <i class="fas fa-keyboard"></i>
                                    Transcription
                                    ${isRealTranscript ? '<small style="color: var(--accent); margin-left: 10px;">(Azure AI Generated)</small>' : ''}
                                </h4>
                                <div>
                                    ${!hasSummary ? `
                                        <button class="action-btn summary" onclick="generateSummary('${item.id}')" style="margin-right: 0.5rem;" ${!azureLanguageAvailable ? 'disabled' : ''}>
                                            <i class="fas fa-file-alt"></i>
                                            ${azureLanguageAvailable ? 'AI Summary' : 'Azure Offline'}
                                        </button>
                                    ` : ''}
                                    <button class="action-btn download" onclick="downloadText('${item.id}', 'transcription', '${(item.fileName || 'meeting')}-transcription.txt')">
                                        <i class="fas fa-download"></i>
                                        Download
                                    </button>
                                </div>
                            </div>
                            <div class="section-content transcription-content">
                                ${item.transcription || 'No transcription available'}
                            </div>
                        </div>
                    ` : ''}

                    ${hasSummary ? `
                        <div class="content-section">
                            <div class="section-header">
                                <h4 class="section-title">
                                    <i class="fas fa-file-alt"></i>
                                    Summary
                                    ${isRealSummary ? '<small style="color: var(--success); margin-left: 10px;">(Azure AI Generated)</small>' : ''}
                                </h4>
                                <button class="action-btn download" onclick="downloadText('${item.id}', 'summary', '${(item.fileName || 'meeting')}-summary.txt')">
                                    <i class="fas fa-download"></i>
                                    Download
                                </button>
                            </div>
                            <div class="section-content summary-content">
                                ${item.summary || 'No summary available'}
                            </div>
                        </div>
                    ` : ''}
                </div>
                `;
            }).join('');

            // Setup audio players after rendering
            setupAudioPlayers();
        }

        // Show format selection modal
        function showFormatSelection(recordingId) {
            recordingToDownload = recordingId;
            formatModal.style.display = 'flex';
        }

        // Close format modal
        function closeFormatModalHandler() {
            formatModal.style.display = 'none';
            recordingToDownload = null;
        }

        // Download as MP4
        async function downloadAsMP4() {
            if (!recordingToDownload) return;
            
            try {
                closeFormatModalHandler();
                
                if (!isFFmpegLoaded) {
                    showNotification('Audio conversion not available for MP4', 'error');
                    return;
                }

                const recordings = JSON.parse(localStorage.getItem('meetingRecordings') || '[]');
                const recording = recordings.find(r => r.id === recordingToDownload);
                const audioData = JSON.parse(localStorage.getItem(`audio_${recordingToDownload}`) || '{}');
                
                if (!audioData.audioData) {
                    showNotification('Recording data not found', 'error');
                    return;
                }

                showNotification('Converting to MP4 format...', 'success');

                // Convert base64 to blob
                const response = await fetch(audioData.audioData);
                const webmBlob = await response.blob();

                // Convert to MP4
                const mp4Blob = await convertWebMToMP4(webmBlob, recordingToDownload);
                
                // Download the MP4 file
                const url = URL.createObjectURL(mp4Blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = `${recording.fileName}.mp4`;
                document.body.appendChild(a);
                a.click();
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
                
                showNotification('MP4 download started!', 'success');
                
            } catch (error) {
                console.error('MP4 conversion/download error:', error);
                showNotification('MP4 conversion failed: ' + error.message, 'error');
            }
        }

        // Download as original WebM
        async function downloadAsWebM() {
            if (!recordingToDownload) return;
            
            try {
                closeFormatModalHandler();
                
                const recordings = JSON.parse(localStorage.getItem('meetingRecordings') || '[]');
                const recording = recordings.find(r => r.id === recordingToDownload);
                const audioData = JSON.parse(localStorage.getItem(`audio_${recordingToDownload}`) || '{}');
                
                if (audioData.audioData) {
                    // Convert base64 back to blob
                    const binaryString = atob(audioData.audioData.split(',')[1]);
                    const bytes = new Uint8Array(binaryString.length);
                    for (let i = 0; i < binaryString.length; i++) {
                        bytes[i] = binaryString.charCodeAt(i);
                    }
                    const blob = new Blob([bytes], { type: 'audio/webm' });
                    
                    const url = URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = `${recording.fileName}.webm`;
                    document.body.appendChild(a);
                    a.click();
                    document.body.removeChild(a);
                    URL.revokeObjectURL(url);
                    
                    showNotification('WebM download started', 'success');
                } else {
                    showNotification('Recording data not found', 'error');
                }
            } catch (error) {
                console.error('WebM download error:', error);
                showNotification('Download failed: ' + error.message, 'error');
            }
        }

        // Setup audio players with actual audio data
        function setupAudioPlayers() {
            recordings.forEach(item => {
                const audioElement = document.getElementById(`audio-${item.id}`);
                if (audioElement) {
                    const audioData = JSON.parse(localStorage.getItem(`audio_${item.id}`) || '{}');
                    if (audioData.audioData) {
                        audioElement.src = audioData.audioData;
                    }
                }
            });
        }

        // Play recording from localStorage
        function playRecording(recordingId) {
            const audioData = JSON.parse(localStorage.getItem(`audio_${recordingId}`) || '{}');
            
            if (audioData.audioData) {
                const audio = new Audio(audioData.audioData);
                audio.play().catch(error => {
                    console.error('Error playing recording:', error);
                    showNotification('Error playing recording', 'error');
                });
                
                showNotification('Playing recording...', 'success');
            } else {
                showNotification('Recording audio data not found', 'error');
            }
        }

        // Transcribe using Azure Speech Services with WebM to WAV conversion
        async function transcribeRecordingWithAzure(recordingId) {
            try {
                const itemElement = document.querySelector(`[data-id="${recordingId}"]`);
                const transcribeBtn = itemElement.querySelector('.action-btn.transcribe');
                
                transcribeBtn.innerHTML = '<span class="loading-spinner"></span> Converting & Transcribing...';
                transcribeBtn.disabled = true;

                // Update UI status
                const statusElement = itemElement.querySelector('.status-badge');
                if (statusElement) {
                    statusElement.className = 'status-badge processing';
                    statusElement.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Converting WebM to WAV';
                }

                showNotification('Converting WebM to WAV for Azure transcription...', 'success');

                // Get audio data from localStorage
                const audioData = JSON.parse(localStorage.getItem(`audio_${recordingId}`) || '{}');
                
                if (!audioData.audioData) {
                    throw new Error('No audio data found for transcription');
                }

                // Convert base64 to WebM blob
                const response = await fetch(audioData.audioData);
                const webmBlob = await response.blob();

                // Convert WebM to WAV
                const wavBlob = await convertWebMToWAVForAzure(webmBlob);

                // Update status
                if (statusElement) {
                    statusElement.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Transcribing with Azure';
                }
                showNotification('WebM converted to WAV. Starting Azure transcription...', 'success');

                // Create WAV file for Azure
                const wavFile = new File([wavBlob], `recording-${recordingId}.wav`, { type: 'audio/wav' });

                // Transcribe using Azure Speech Services
                const transcription = await transcribeAudioWithAzure(wavFile);
                
                if (transcription) {
                    // Update the recording with actual transcription
                    const recordings = JSON.parse(localStorage.getItem('meetingRecordings') || '[]');
                    const recordingIndex = recordings.findIndex(r => r.id === recordingId);
                    
                    if (recordingIndex !== -1) {
                        recordings[recordingIndex].transcription = transcription;
                        recordings[recordingIndex].isRealTranscript = true;
                        recordings[recordingIndex].transcriptionModel = 'azure-speech';
                        
                        localStorage.setItem('meetingRecordings', JSON.stringify(recordings));
                        
                        showNotification('✅ Transcription completed using Azure Speech Services!', 'success');
                        loadHistory();
                    }
                } else {
                    throw new Error('Transcription returned empty result');
                }

            } catch (error) {
                console.error('Azure transcription error:', error);
                showNotification('❌ Azure transcription failed: ' + error.message, 'error');
                
                // Reset button
                resetTranscribeButton(recordingId);
            }
        }

        // Azure Speech Services transcription function
        async function transcribeAudioWithAzure(audioFile) {
            return new Promise(async (resolve, reject) => {
                try {
                    // Check if Speech SDK is available
                    if (typeof SpeechSDK === 'undefined') {
                        reject(new Error('Azure Speech SDK not loaded'));
                        return;
                    }

                    // Use the first Azure key
                    const subscriptionKey = AZURE_KEYS.key1;
                    
                    // Create speech configuration
                    const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(subscriptionKey, AZURE_REGION);
                    speechConfig.speechRecognitionLanguage = "en-US";

                    // Create audio configuration from the WAV file
                    const audioConfig = SpeechSDK.AudioConfig.fromWavFileInput(audioFile);

                    // Create speech recognizer
                    const recognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);

                    let fullTranscript = '';

                    recognizer.recognizing = (s, e) => {
                        console.log(`Recognizing: ${e.result.text}`);
                    };

                    recognizer.recognized = (s, e) => {
                        if (e.result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
                            console.log(`Recognized: ${e.result.text}`);
                            fullTranscript += e.result.text + ' ';
                        }
                    };

                    recognizer.canceled = (s, e) => {
                        console.log(`Canceled: ${e.reason}`);
                        if (e.reason === SpeechSDK.CancellationReason.Error) {
                            console.log(`Error details: ${e.errorDetails}`);
                            reject(new Error(`Azure Speech recognition error: ${e.errorDetails}`));
                        }
                        recognizer.close();
                    };

                    recognizer.sessionStopped = (s, e) => {
                        console.log("Session stopped");
                        recognizer.close();
                        resolve(fullTranscript.trim());
                    };

                    // Start continuous recognition
                    recognizer.startContinuousRecognitionAsync(
                        () => {
                            console.log("Azure Speech recognition started");
                            // Stop recognition after 60 seconds to prevent hanging
                            setTimeout(() => {
                                recognizer.stopContinuousRecognitionAsync(
                                    () => {
                                        console.log("Azure Speech recognition stopped (timeout)");
                                        resolve(fullTranscript.trim());
                                    },
                                    (err) => {
                                        console.error("Error stopping recognition:", err);
                                        resolve(fullTranscript.trim());
                                    }
                                );
                            }, 60000);
                        },
                        (err) => {
                            console.error("Error starting recognition:", err);
                            reject(new Error(`Failed to start Azure Speech recognition: ${err}`));
                        }
                    );

                } catch (error) {
                    console.error('Azure Speech Services error:', error);
                    reject(error);
                }
            });
        }

        function resetTranscribeButton(recordingId) {
            const itemElement = document.querySelector(`[data-id="${recordingId}"]`);
            if (itemElement) {
                const transcribeBtn = itemElement.querySelector('.action-btn.transcribe');
                if (transcribeBtn) {
                    transcribeBtn.innerHTML = '<i class="fas fa-keyboard"></i> Transcribe';
                    transcribeBtn.disabled = false;
                }
                
                // Reset status badge
                const statusElement = itemElement.querySelector('.status-badge');
                if (statusElement && statusElement.classList.contains('processing')) {
                    statusElement.className = 'status-badge pending';
                    statusElement.innerHTML = '<i class="fas fa-microphone"></i> Recorded';
                }
            }
        }

        // Check Azure Speech Services status
        async function checkAzureSpeechStatus() {
            try {
                // Simple check to see if Azure Speech SDK is available
                if (typeof SpeechSDK !== 'undefined') {
                    azureSpeechAvailable = true;
                    azureStatus.className = 'azure-status connected';
                    azureStatus.innerHTML = `
                        <i class="fas fa-check-circle"></i>
                        <span>Azure Speech Services connected - Ready for transcription</span>
                    `;
                    console.log('✅ Azure Speech Services SDK loaded');
                } else {
                    azureSpeechAvailable = false;
                    azureStatus.className = 'azure-status disconnected';
                    azureStatus.innerHTML = `
                        <i class="fas fa-exclamation-triangle"></i>
                        <span>Azure Speech Services SDK not loaded</span>
                    `;
                }
            } catch (error) {
                azureSpeechAvailable = false;
                azureStatus.className = 'azure-status disconnected';
                azureStatus.innerHTML = `
                    <i class="fas fa-exclamation-triangle"></i>
                    <span>Azure Speech Services unavailable</span>
                `;
                console.log('❌ Azure Speech Services check failed');
            }
        }

        // Check Azure Language Service status
        async function checkAzureLanguageStatus() {
            try {
                // Test the Azure Language Service connection
                if (AZURE_LANGUAGE_KEY && AZURE_LANGUAGE_ENDPOINT) {
                    azureLanguageAvailable = true;
                    azureLanguageStatus.className = 'azure-status connected';
                    azureLanguageStatus.innerHTML = `
                        <i class="fas fa-check-circle"></i>
                        <span>Azure Language Service connected - Ready for AI summarization</span>
                    `;
                    console.log('✅ Azure Language Service configured');
                } else {
                    azureLanguageAvailable = false;
                    azureLanguageStatus.className = 'azure-status disconnected';
                    azureLanguageStatus.innerHTML = `
                        <i class="fas fa-exclamation-triangle"></i>
                        <span>Azure Language Service not configured</span>
                    `;
                }
            } catch (error) {
                azureLanguageAvailable = false;
                azureLanguageStatus.className = 'azure-status disconnected';
                azureLanguageStatus.innerHTML = `
                    <i class="fas fa-exclamation-triangle"></i>
                    <span>Azure Language Service unavailable</span>
                `;
                console.log('❌ Azure Language Service check failed');
            }
        }

        // Generate summary using Azure AI or local processing
        async function generateSummary(recordingId) {
            try {
                const itemElement = document.querySelector(`[data-id="${recordingId}"]`);
                const summaryBtn = itemElement.querySelector('.action-btn.summary');
                
                summaryBtn.innerHTML = '<span class="loading-spinner"></span> Generating Summary...';
                summaryBtn.disabled = true;

                showNotification('Generating AI summary from transcription...', 'success');

                // Get the transcription
                const recordings = JSON.parse(localStorage.getItem('meetingRecordings') || '[]');
                const recordingIndex = recordings.findIndex(r => r.id === recordingId);
                
                if (recordingIndex === -1) {
                    throw new Error('Recording not found');
                }

                const transcription = recordings[recordingIndex].transcription;
                
                if (!transcription || transcription.trim() === '') {
                    throw new Error('No transcription available to summarize');
                }

                // Generate summary using Azure AI or local processing
                const summary = await generateSummaryFromTranscription(transcription);
                
                if (summary) {
                    recordings[recordingIndex].summary = summary;
                    recordings[recordingIndex].isRealSummary = true;
                    localStorage.setItem('meetingRecordings', JSON.stringify(recordings));
                    
                    showNotification('AI summary generated successfully!', 'success');
                    loadHistory();
                } else {
                    throw new Error('Summary generation returned empty result');
                }

            } catch (error) {
                console.error('Summary generation error:', error);
                showNotification('Summary generation failed: ' + error.message, 'error');
                
                // Reset button
                const itemElement = document.querySelector(`[data-id="${recordingId}"]`);
                if (itemElement) {
                    const summaryBtn = itemElement.querySelector('.action-btn.summary');
                    if (summaryBtn) {
                        summaryBtn.innerHTML = '<i class="fas fa-file-alt"></i> AI Summary';
                        summaryBtn.disabled = false;
                    }
                }
            }
        }

        // Generate summary from transcription using Azure AI or local processing
        async function generateSummaryFromTranscription(transcription) {
            try {
                // Always try Azure Language Service first if available
                if (azureLanguageAvailable) {
                    try {
                        const azureSummary = await generateSummaryWithAzureAI(transcription);
                        if (azureSummary && azureSummary.trim() !== '') {
                            return azureSummary;
                        }
                    } catch (azureError) {
                        console.log('Azure AI summary failed, using local processing:', azureError);
                        showNotification('Azure AI service busy, using local processing', 'warning');
                    }
                }
                
                // Fallback to local processing
                return generateSummaryLocally(transcription);
                
            } catch (error) {
                console.error('Summary generation error:', error);
                throw error;
            }
        }

        // Generate summary using Azure Language Service
        async function generateSummaryWithAzureAI(transcription) {
            try {
                showNotification('Using Azure AI for advanced summarization...', 'success');

                // Azure Language Service API endpoint for summarization
                const endpoint = `${AZURE_LANGUAGE_ENDPOINT}language/analyze-text/jobs?api-version=2023-04-01`;
                
                // Prepare the request body for extractive summarization
                const requestBody = {
                    displayName: "Meeting Summary",
                    analysisInput: {
                        documents: [
                            {
                                id: "1",
                                language: "en",
                                text: transcription.substring(0, 125000) // Limit for API
                            }
                        ]
                    },
                    tasks: [
                        {
                            kind: "ExtractiveSummarization",
                            taskName: "Extract Meeting Summary",
                            parameters: {
                                modelVersion: "latest",
                                sentenceCount: 7,
                                sortBy: "Offset" // Or "Rank" for importance-based sorting
                            }
                        }
                    ]
                };

                const response = await fetch(endpoint, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Ocp-Apim-Subscription-Key': AZURE_LANGUAGE_KEY
                    },
                    body: JSON.stringify(requestBody)
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`Azure API error: ${response.status} - ${errorText}`);
                }

                const result = await response.json();
                
                // For async operations, we might need to poll for results
                // For now, let's assume synchronous response
                if (result.tasks && result.tasks.items && result.tasks.items[0]) {
                    const task = result.tasks.items[0];
                    if (task.results && task.results.documents && task.results.documents[0]) {
                        const summaryDoc = task.results.documents[0];
                        if (summaryDoc.sentences && summaryDoc.sentences.length > 0) {
                            const summarySentences = summaryDoc.sentences
                                .map(sentence => sentence.text)
                                .join(' ');
                            
                            return formatAzureSummary(summarySentences, transcription);
                        }
                    }
                }
                
                throw new Error('No summary generated from Azure');
                
            } catch (error) {
                console.error('Azure Language Service error:', error);
                throw new Error(`Azure summarization failed: ${error.message}`);
            }
        }

        // Format the Azure summary with additional context
        function formatAzureSummary(extractedSummary, fullTranscription) {
            const sentences = extractedSummary.split('. ').filter(s => s.trim().length > 0);
            
            let formattedSummary = "AI-POWERED MEETING SUMMARY (Azure AI Generated)\n\n";
            formattedSummary += "Key Discussion Points:\n";
            
            sentences.forEach((sentence, index) => {
                formattedSummary += `${index + 1}. ${sentence.trim()}\n`;
            });
            
            // Add analysis metrics
            const wordCount = fullTranscription.split(' ').length;
            const summaryWordCount = extractedSummary.split(' ').length;
            const coverage = Math.round((summaryWordCount / wordCount) * 100);
            
            formattedSummary += `\nSummary Analysis:\n`;
            formattedSummary += `• Original transcription: ${wordCount} words\n`;
            formattedSummary += `• Summary coverage: ${coverage}% of key content\n`;
            formattedSummary += `• AI-extracted ${sentences.length} key sentences\n`;
            
            formattedSummary += "\nFull transcription available for detailed review of all discussion points.";
            
            return formattedSummary;
        }

        // Generate summary using local processing
        function generateSummaryLocally(transcription) {
            // Simple local summarization algorithm
            const sentences = transcription.split(/[.!?]+/).filter(s => s.trim().length > 0);
            
            // Extract key sentences (first few sentences and sentences with important keywords)
            const keySentences = [];
            const importantKeywords = ['important', 'key', 'summary', 'conclusion', 'decision', 'action', 'next', 'deadline', 'goal'];
            
            // Add first sentence (usually contains main topic)
            if (sentences.length > 0) {
                keySentences.push(sentences[0].trim());
            }
            
            // Add sentences with important keywords
            for (let i = 1; i < sentences.length && keySentences.length < 5; i++) {
                const sentence = sentences[i].toLowerCase();
                if (importantKeywords.some(keyword => sentence.includes(keyword))) {
                    keySentences.push(sentences[i].trim());
                }
            }
            
            // If not enough key sentences, add more from the middle
            if (keySentences.length < 3 && sentences.length > 2) {
                const midIndex = Math.floor(sentences.length / 2);
                keySentences.push(sentences[midIndex].trim());
            }
            
            // Create summary
            let summary = "MEETING SUMMARY (Local Processing)\n\n";
            summary += "Key Points Discussed:\n";
            
            keySentences.forEach((sentence, index) => {
                summary += `• ${sentence}\n`;
            });
            
            summary += "\nMain Topics:\n";
            
            // Extract topics from first few sentences
            const topics = sentences.slice(0, 3).map(s => {
                const words = s.split(' ').slice(0, 5).join(' ');
                return `• ${words}...`;
            });
            
            summary += topics.join('\n');
            
            summary += "\n\nAction Items:\n";
            summary += "1. Review decisions made during the meeting\n";
            summary += "2. Follow up on action items discussed\n";
            summary += "3. Schedule next steps if required\n";
            
            summary += "\nNext Steps: Review the full transcription for detailed discussion points.";
            
            return summary;
        }

        // Download text content
        function downloadText(recordingId, type, fileName) {
            const recordings = JSON.parse(localStorage.getItem('meetingRecordings') || '[]');
            const recording = recordings.find(r => r.id === recordingId);
            if (!recording) return;
            
            let content = '';
            
            if (type === 'transcription') {
                content = recording.transcription || '';
            } else if (type === 'summary') {
                content = recording.summary || '';
            }

            if (!content) {
                showNotification('No content to download', 'warning');
                return;
            }

            const blob = new Blob([content], { type: 'text/plain' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = fileName;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);

            showNotification('Download started', 'success');
        }

        // Highlight selected recording when coming from meeting page
        function highlightSelectedRecording() {
            const selectedRecordingId = localStorage.getItem('selectedRecording');
            if (selectedRecordingId) {
                const element = document.getElementById(`recording-${selectedRecordingId}`);
                if (element) {
                    element.scrollIntoView({ behavior: 'smooth', block: 'center' });
                    element.style.background = 'linear-gradient(135deg, #e0f7fa 0%, #bbdefb 100%)';
                    element.style.borderLeft = '4px solid #00b4db';
                    
                    // Remove highlight after 3 seconds
                    setTimeout(() => {
                        element.style.background = '';
                        element.style.borderLeft = '4px solid var(--accent)';
                    }, 3000);
                }
                localStorage.removeItem('selectedRecording');
            }
        }

        // Delete recording functions
        function confirmDeleteRecording(recordingId) {
            recordingToDelete = recordingId;
            deleteModal.style.display = 'flex';
        }

        function closeDeleteModalHandler() {
            deleteModal.style.display = 'none';
            recordingToDelete = null;
        }

        function deleteRecordingConfirmed() {
            if (!recordingToDelete) return;

            try {
                // Remove from recordings list
                let recordings = JSON.parse(localStorage.getItem('meetingRecordings') || '[]');
                recordings = recordings.filter(r => r.id !== recordingToDelete);
                localStorage.setItem('meetingRecordings', JSON.stringify(recordings));
                
                // Remove audio data
                localStorage.removeItem(`audio_${recordingToDelete}`);
                
                showNotification('Recording deleted successfully', 'success');
                closeDeleteModalHandler();
                loadHistory();
                
            } catch (error) {
                console.error('Delete error:', error);
                showNotification('Failed to delete recording', 'error');
                closeDeleteModalHandler();
            }
        }

        // Utility functions
        function formatTime(seconds) {
            if (!seconds) return '00:00';
            const hrs = Math.floor(seconds / 3600);
            const mins = Math.floor((seconds % 3600) / 60);
            const secs = seconds % 60;
            return `${hrs.toString().padStart(2, '0')}:${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
        }

        function formatDate(dateString) {
            if (!dateString) return 'Unknown date';
            const date = new Date(dateString);
            return date.toLocaleDateString() + ' ' + date.toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'});
        }

        function formatFileSize(bytes) {
            if (!bytes || bytes === 0) return '0 Bytes';
            const k = 1024;
            const sizes = ['Bytes', 'KB', 'MB', 'GB'];
            const i = Math.floor(Math.log(bytes) / Math.log(k));
            return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
        }

        function showNotification(message, type) {
            notification.innerHTML = `
                <i class="fas fa-${type === 'success' ? 'check-circle' : type === 'error' ? 'exclamation-circle' : 'exclamation-triangle'}"></i>
                ${message}
            `;
            notification.className = `notification ${type}`;
            
            setTimeout(() => {
                notification.classList.remove('hidden');
            }, 100);
            
            setTimeout(() => {
                notification.classList.add('hidden');
            }, 3000);
        }
    </script>
</body>
</html>